{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Coefficients\n",
    "\n",
    "Linear regression predicts a quantitative response Y on the basis of predictor variables X1, X2, ... Xn. It assumes that there is a linear relationship between X(s) and Y. Mathematically, we write this linear relationship as Y ≈ β0 + β1X1 + β2X2 + ... + βnXn.\n",
    "\n",
    "**The magnitude of the coefficients is directly influenced by the scale of the features**. Therefore, to compare coefficients across features, it is importance that all features are within a similar scale. This is why, normalisation is important for variable importance and feature selection in linear models.\n",
    "\n",
    "Linear Regression makes the following assumptions over the predictor variables X:\n",
    "- Linear relationship with the outcome Y\n",
    "- Multivariate normality (X should follow a Gaussian distribution)\n",
    "- No or little multicollinearity (Xs should not be linearly related to one another)\n",
    "- Homoscedasticity (variance should be the same)\n",
    "\n",
    "Homoscedasticity, also known as homogeneity of variance, describes a situation in which the error term (that is, the “noise” or random disturbance in the relationship between the independent variables (Xs) and the dependent variable (Y)) is the same across all values of the independent variables.\n",
    "\n",
    "Therefore, there are a lot of assumptions that need to be met in order to make a fair comparison of the features by using only their regression coefficients.\n",
    "\n",
    "In addition, these coefficients may be penalised by regularisation, therefore being smaller than if we were to compare the relationship of each feature with the target individually.\n",
    "\n",
    "Having said this, you can still select features based on linear regression coefficients, provided you keep all of these in mind at the time of analysing the outcome.\n",
    "\n",
    "Personally, this is not my feature selection method of choice, although I find it useful to interpret the output of the model.\n",
    "\n",
    "I will demonstrate how to select features based in a regression and a classification scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 109)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "data = pd.read_csv('../Data/dataset_2.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>var_10</th>\n",
       "      <th>...</th>\n",
       "      <th>var_100</th>\n",
       "      <th>var_101</th>\n",
       "      <th>var_102</th>\n",
       "      <th>var_103</th>\n",
       "      <th>var_104</th>\n",
       "      <th>var_105</th>\n",
       "      <th>var_106</th>\n",
       "      <th>var_107</th>\n",
       "      <th>var_108</th>\n",
       "      <th>var_109</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.532710</td>\n",
       "      <td>3.280834</td>\n",
       "      <td>17.982476</td>\n",
       "      <td>4.404259</td>\n",
       "      <td>2.349910</td>\n",
       "      <td>0.603264</td>\n",
       "      <td>2.784655</td>\n",
       "      <td>0.323146</td>\n",
       "      <td>12.009691</td>\n",
       "      <td>0.139346</td>\n",
       "      <td>...</td>\n",
       "      <td>2.079066</td>\n",
       "      <td>6.748819</td>\n",
       "      <td>2.941445</td>\n",
       "      <td>18.360496</td>\n",
       "      <td>17.726613</td>\n",
       "      <td>7.774031</td>\n",
       "      <td>1.473441</td>\n",
       "      <td>1.973832</td>\n",
       "      <td>0.976806</td>\n",
       "      <td>2.541417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.821374</td>\n",
       "      <td>12.098722</td>\n",
       "      <td>13.309151</td>\n",
       "      <td>4.125599</td>\n",
       "      <td>1.045386</td>\n",
       "      <td>1.832035</td>\n",
       "      <td>1.833494</td>\n",
       "      <td>0.709090</td>\n",
       "      <td>8.652883</td>\n",
       "      <td>0.102757</td>\n",
       "      <td>...</td>\n",
       "      <td>2.479789</td>\n",
       "      <td>7.795290</td>\n",
       "      <td>3.557890</td>\n",
       "      <td>17.383378</td>\n",
       "      <td>15.193423</td>\n",
       "      <td>8.263673</td>\n",
       "      <td>1.878108</td>\n",
       "      <td>0.567939</td>\n",
       "      <td>1.018818</td>\n",
       "      <td>1.416433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.938776</td>\n",
       "      <td>7.952752</td>\n",
       "      <td>0.972671</td>\n",
       "      <td>3.459267</td>\n",
       "      <td>1.935782</td>\n",
       "      <td>0.621463</td>\n",
       "      <td>2.338139</td>\n",
       "      <td>0.344948</td>\n",
       "      <td>9.937850</td>\n",
       "      <td>11.691283</td>\n",
       "      <td>...</td>\n",
       "      <td>1.861487</td>\n",
       "      <td>6.130886</td>\n",
       "      <td>3.401064</td>\n",
       "      <td>15.850471</td>\n",
       "      <td>14.620599</td>\n",
       "      <td>6.849776</td>\n",
       "      <td>1.098210</td>\n",
       "      <td>1.959183</td>\n",
       "      <td>1.575493</td>\n",
       "      <td>1.857893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.020690</td>\n",
       "      <td>9.900544</td>\n",
       "      <td>17.869637</td>\n",
       "      <td>4.366715</td>\n",
       "      <td>1.973693</td>\n",
       "      <td>2.026012</td>\n",
       "      <td>2.853025</td>\n",
       "      <td>0.674847</td>\n",
       "      <td>11.816859</td>\n",
       "      <td>0.011151</td>\n",
       "      <td>...</td>\n",
       "      <td>1.340944</td>\n",
       "      <td>7.240058</td>\n",
       "      <td>2.417235</td>\n",
       "      <td>15.194609</td>\n",
       "      <td>13.553772</td>\n",
       "      <td>7.229971</td>\n",
       "      <td>0.835158</td>\n",
       "      <td>2.234482</td>\n",
       "      <td>0.946170</td>\n",
       "      <td>2.700606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.909506</td>\n",
       "      <td>10.576516</td>\n",
       "      <td>0.934191</td>\n",
       "      <td>3.419572</td>\n",
       "      <td>1.871438</td>\n",
       "      <td>3.340811</td>\n",
       "      <td>1.868282</td>\n",
       "      <td>0.439865</td>\n",
       "      <td>13.585620</td>\n",
       "      <td>1.153366</td>\n",
       "      <td>...</td>\n",
       "      <td>2.738095</td>\n",
       "      <td>6.565509</td>\n",
       "      <td>4.341414</td>\n",
       "      <td>15.893832</td>\n",
       "      <td>11.929787</td>\n",
       "      <td>6.954033</td>\n",
       "      <td>1.853364</td>\n",
       "      <td>0.511027</td>\n",
       "      <td>2.599562</td>\n",
       "      <td>0.811364</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      var_1      var_2      var_3     var_4     var_5     var_6     var_7  \\\n",
       "0  4.532710   3.280834  17.982476  4.404259  2.349910  0.603264  2.784655   \n",
       "1  5.821374  12.098722  13.309151  4.125599  1.045386  1.832035  1.833494   \n",
       "2  1.938776   7.952752   0.972671  3.459267  1.935782  0.621463  2.338139   \n",
       "3  6.020690   9.900544  17.869637  4.366715  1.973693  2.026012  2.853025   \n",
       "4  3.909506  10.576516   0.934191  3.419572  1.871438  3.340811  1.868282   \n",
       "\n",
       "      var_8      var_9     var_10  ...   var_100   var_101   var_102  \\\n",
       "0  0.323146  12.009691   0.139346  ...  2.079066  6.748819  2.941445   \n",
       "1  0.709090   8.652883   0.102757  ...  2.479789  7.795290  3.557890   \n",
       "2  0.344948   9.937850  11.691283  ...  1.861487  6.130886  3.401064   \n",
       "3  0.674847  11.816859   0.011151  ...  1.340944  7.240058  2.417235   \n",
       "4  0.439865  13.585620   1.153366  ...  2.738095  6.565509  4.341414   \n",
       "\n",
       "     var_103    var_104   var_105   var_106   var_107   var_108   var_109  \n",
       "0  18.360496  17.726613  7.774031  1.473441  1.973832  0.976806  2.541417  \n",
       "1  17.383378  15.193423  8.263673  1.878108  0.567939  1.018818  1.416433  \n",
       "2  15.850471  14.620599  6.849776  1.098210  1.959183  1.575493  1.857893  \n",
       "3  15.194609  13.553772  7.229971  0.835158  2.234482  0.946170  2.700606  \n",
       "4  15.893832  11.929787  6.954033  1.853364  0.511027  2.599562  0.811364  \n",
       "\n",
       "[5 rows x 109 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**\n",
    "\n",
    "In all feature selection procedures, it is good practice to select the features by examining only the training set. And this is to avoid overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35000, 108), (15000, 108))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['target'], axis=1),\n",
    "    data['target'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will scale the variables, so we fit a scaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectFromModel(estimator=LogisticRegression(C=1000, max_iter=300,\n",
       "                                             random_state=10))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here I will do the model fitting and feature selection\n",
    "# altogether in 2 lines of code\n",
    "\n",
    "# first I specify the Logistic Regression model, here I\n",
    "# select the Ridge Penalty (l2)(it is the default parameter in sklearn)\n",
    "\n",
    "# remember that here I want to evaluate the coefficient magnitud\n",
    "# itself and not whether lasso shrinks coefficients to zero\n",
    "\n",
    "# ideally, I want to avoid regularisation at all, so the coefficients\n",
    "# are not affected (modified) by the penalty of the regularisation\n",
    "\n",
    "# In order to do this in sklearn, I set the parameter C really high\n",
    "# which is basically like fitting a non-regularised logistic regression\n",
    "\n",
    "# Then I use the selectFromModel object from sklearn\n",
    "# to automatically select the features\n",
    "\n",
    "# set C to 1000, to avoid regularisation\n",
    "sel_ = SelectFromModel(\n",
    "    LogisticRegression(C=1000, penalty='l2', max_iter=300, random_state=10))\n",
    "\n",
    "sel_.fit(scaler.transform(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True, False, False, False, False, False, False,\n",
       "       False,  True, False, False, False, False, False, False, False,\n",
       "       False, False,  True, False,  True,  True, False,  True, False,\n",
       "       False, False, False, False,  True,  True, False, False, False,\n",
       "       False, False,  True,  True, False, False, False, False, False,\n",
       "       False, False,  True, False,  True, False,  True, False, False,\n",
       "        True,  True, False, False, False,  True, False,  True, False,\n",
       "       False, False, False, False,  True,  True, False,  True, False,\n",
       "        True, False, False,  True, False, False,  True, False, False,\n",
       "        True,  True, False, False, False,  True,  True, False,  True,\n",
       "       False,  True, False, False, False, False,  True, False,  True,\n",
       "       False,  True, False,  True, False,  True, False, False, False])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this command let's me visualise those features that were kept.\n",
    "\n",
    "# sklearn will select those features which coefficients are greater\n",
    "# than the mean of all the coefficients.\n",
    "\n",
    "# it compares absolute values of coefficients. More on this in a second.\n",
    "\n",
    "sel_.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's add the variable names and order it for clearer visualisation\n",
    "# and then let's sum the number of selected features\n",
    "\n",
    "selected_feat = X_train.columns[(sel_.get_support())]\n",
    "len(selected_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.15537943e-02, -1.34485946e-02,  1.08325635e-01,\n",
       "         2.65195774e-02,  4.34476405e-02, -2.45601720e-02,\n",
       "        -3.33519477e-02, -6.17605040e-03,  2.32512356e-02,\n",
       "        -9.79083063e-03,  7.96454987e-02,  5.29740553e-02,\n",
       "        -3.75540843e-02,  1.92853235e-02, -3.86160707e-02,\n",
       "         2.86860983e-02,  1.37573343e-02,  5.43636596e-02,\n",
       "         1.28906552e-02,  1.54099068e-02,  1.41981357e-01,\n",
       "         2.51150875e-02, -1.28455161e-01,  1.26670632e-01,\n",
       "         1.44163127e-02,  6.34707578e-02,  2.75965226e-02,\n",
       "        -8.16372553e-04, -1.41596274e-02, -1.42440295e-02,\n",
       "         6.85080708e-03, -1.86767054e-01, -1.00512077e-01,\n",
       "         3.25439953e-02,  1.18524726e-02,  4.36708214e-02,\n",
       "         1.96738927e-02,  4.49206305e-02,  7.90667692e-02,\n",
       "         7.02871100e-02, -3.38711156e-03, -2.18175430e-03,\n",
       "        -5.34783669e-02,  5.64144530e-02,  3.63998852e-03,\n",
       "         3.05024137e-02,  4.94727156e-03,  4.21350482e-01,\n",
       "         2.14850323e-02,  7.34922379e-02,  1.49659954e-02,\n",
       "         6.87043219e-02,  4.50996666e-04,  8.84700895e-03,\n",
       "         8.26991601e-01, -1.04755860e-01, -5.79758783e-03,\n",
       "         2.90298683e-02,  1.11835113e-02, -6.75409542e-02,\n",
       "         1.78245739e-02, -1.88030717e-01,  1.58664622e-02,\n",
       "        -4.12503068e-03,  2.66819202e-02,  1.91412082e-02,\n",
       "         3.96824687e-03, -3.08196868e-01, -1.69798896e-01,\n",
       "         2.22835472e-03, -7.83653904e-02,  3.51553982e-03,\n",
       "        -7.67217975e-02,  7.11480940e-03, -4.63603793e-02,\n",
       "         9.65856086e-02, -3.42539221e-03, -1.20575963e-03,\n",
       "         1.24997702e-01, -1.34964926e-02,  7.92042542e-03,\n",
       "         1.49790786e-01,  6.22343936e-02,  8.53989096e-03,\n",
       "         3.35010285e-02, -3.84157357e-03, -1.60375108e-01,\n",
       "        -5.71042723e-02, -3.17623319e-02, -7.22518837e-02,\n",
       "        -5.14681092e-03,  5.93244252e-02,  4.91575108e-02,\n",
       "        -4.41809402e-02,  2.34822949e-02,  1.90714088e-02,\n",
       "        -9.35264785e-02,  2.32721100e-03,  5.84592579e-02,\n",
       "         2.70544522e-02,  1.10307128e-01, -6.73206854e-03,\n",
       "        -5.69488793e-02,  1.05354819e-02, -9.08831911e-02,\n",
       "        -2.72644374e-03, -5.61316477e-02,  8.99876185e-03]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with the parameter coef_ we access the coefficients of the variables\n",
    "# for the linear regression (for all the 108 variables)\n",
    "\n",
    "sel_.estimator_.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012054903617007643"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as SelectFromModel selects coefficients above the mean\n",
    "# of all coefficients, let's calculate first the mean\n",
    "\n",
    "sel_.estimator_.coef_.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# and now let's plot the distribution of coefficients\n",
    "\n",
    "pd.Series(sel_.estimator_.coef_.ravel()).hist(bins=20)\n",
    "plt.xlabel('Coefficients')\n",
    "plt.ylabel('Number of variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, some coefficients are positive and some are negative, suggesting that some features are negatively associated with the outcome (the more of the feature the less of the outcome) and viceversa.\n",
    "\n",
    "However, the absolute value of the coefficients inform about the importance of the feature on the outcome, and not the sign. Therefore, the feature selection is done filtering on absolute values of coefficients. See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05681290471651183"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the feature importance is informed by the absolute value of\n",
    "# the coefficient, and not the sign.\n",
    "# therefore, let's recalculate the mean using the absolute values instead\n",
    "\n",
    "np.abs(sel_.estimator_.coef_).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# and now let's plot the histogram of absolute coefficients\n",
    "\n",
    "pd.Series(np.abs(sel_.estimator_.coef_).ravel()).hist(bins=20)\n",
    "plt.xlabel('Coefficients')\n",
    "plt.ylabel('Number of variables')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total features: 108\n",
      "selected features: 34\n",
      "features with coefficients greater than the mean coefficient: 34\n"
     ]
    }
   ],
   "source": [
    "# and now, let's compare the  number of selected features\n",
    "# with the number of features which coefficient is above the\n",
    "# mean coefficient, to make sure we understand the output of\n",
    "# SelectFromModel\n",
    "\n",
    "print('total features: {}'.format((X_train.shape[1])))\n",
    "\n",
    "print('selected features: {}'.format(len(selected_feat)))\n",
    "\n",
    "print(\n",
    "    'features with coefficients greater than the mean coefficient: {}'.format(\n",
    "        np.sum(\n",
    "            np.abs(sel_.estimator_.coef_) > np.abs(\n",
    "                sel_.estimator_.coef_).mean())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, we see how select from model works. It will select all the coefficients which absolute values are greater than the mean.\n",
    "\n",
    "To do this, you need to change the default value of the parameter threshold that can be passed to SelectFromModel. More details in the Scikit-learn documentation: https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFromModel.html\n",
    "\n",
    "This is all for this lecture. See you in the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
